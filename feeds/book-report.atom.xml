<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Donnie Ahmad - Book Report</title><link href="/" rel="alternate"></link><link href="/feeds/book-report.atom.xml" rel="self"></link><id>/</id><updated>2018-12-14T18:21:00-07:00</updated><entry><title>Notes on a JMLR Paper</title><link href="/notes-on-a-jmlr-paper.html" rel="alternate"></link><published>2018-12-14T18:21:00-07:00</published><updated>2018-12-14T18:21:00-07:00</updated><author><name>Donnie Ahmad</name></author><id>tag:None,2018-12-14:/notes-on-a-jmlr-paper.html</id><summary type="html">&lt;h1&gt;Numerical Anaylsis near Singularities in RBF Networks&lt;/h1&gt;
&lt;p&gt;Follow along with the source material &lt;a href="http://www.jmlr.org/papers/volume19/16-210/16-210.pdf"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;The parameter space of most learning machines have singular regions where the Fisher information matrices degenerate.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A parameter space is the set of all possible combinations of values for all parameters in a model&lt;a href="https://en.wikipedia.org/wiki/Parameter_space"&gt;.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A parameter â€¦&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1&gt;Numerical Anaylsis near Singularities in RBF Networks&lt;/h1&gt;
&lt;p&gt;Follow along with the source material &lt;a href="http://www.jmlr.org/papers/volume19/16-210/16-210.pdf"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;The parameter space of most learning machines have singular regions where the Fisher information matrices degenerate.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A parameter space is the set of all possible combinations of values for all parameters in a model&lt;a href="https://en.wikipedia.org/wiki/Parameter_space"&gt;.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A parameter differs from a variable in a model is that they tend to represent inherent properties of a system&lt;a href="https://en.wikipedia.org/wiki/Parameter"&gt;.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Learning machine examples provide and brief explanations&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Layered neural networks: used to define a complex, non-linear form of hypotheses using "neurons" that have their input and output mapped through operations called activation functions&lt;a href="http://deeplearning.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/"&gt;.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Normal mixtures: where the joint probability of two random variables is normally distributed&lt;a href="https://www.jmp.com/support/help/14/normal-mixtures.shtml"&gt;.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binomial mixtures&lt;/li&gt;
&lt;li&gt;Bayes networks: represents variables and their "conditional dependencies" in an ordered graphical model&lt;a href="https://en.wikipedia.org/wiki/Bayesian_network"&gt;.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hidden Markov models: a Markov process in an unobserved space&lt;a href="https://en.wikipedia.org/wiki/Hidden_Markov_Models"&gt;.&lt;/a&gt; A Markov chain describes a sequence of events in which the current event relies entirely on the position the previous event created&lt;a href="https://en.wikipedia.org/wiki/Markov_chain"&gt;.&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;Boltzmann machines: a type of stochastic recurrent neural network&lt;a href="https://en.wikipedia.org/wiki/Boltzmann_machine"&gt;.&lt;/a&gt; Stochastic neural networks use random variations to help escape local minima in optimization problems&lt;a href="https://en.wikipedia.org/wiki/Stochastic_neural_network"&gt;.&lt;/a&gt; Recurrent neural networks are where connections between nodes form a directed graph along a sequence&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;.&lt;/a&gt; A directed graph differs from an undirected graph in that the nodes (or edges) are ordered&lt;a href="https://en.wikipedia.org/wiki/Directed_graph"&gt;.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stochastic context-free grammars: a set of rules that describe all possible strings in a language&lt;a href="https://en.wikipedia.org/wiki/Context-free_grammar"&gt;.&lt;/a&gt; As noted above, stochastic refers to the introduced randomness.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A singular region is where a mathematical object is undefined or where the function "misbehaves." A simple example is that f(x) = 1/x has a singularity at x = 0[.](https://en.wikipedia.org/wiki/Singularity_(mathematics)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fisher information is a measurement of how much information a random variable has about an unknown parameter&lt;a href="https://en.wikipedia.org/wiki/Fisher_information"&gt;.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A matrix "degenerates" in that it becomes a singular matrix. Degenerate and singular matrices are defined as not having an inverse&lt;a href="https://math.stackexchange.com/questions/792587/what-is-the-difference-between-singular-matrices-and-degenerate-matrices"&gt;.&lt;/a&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>