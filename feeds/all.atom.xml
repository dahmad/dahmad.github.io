<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Donnie Ahmad</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2018-12-14T18:21:00-07:00</updated><entry><title>Notes on a JMLR Paper</title><link href="/notes-on-a-jmlr-paper.html" rel="alternate"></link><published>2018-12-14T18:21:00-07:00</published><updated>2018-12-14T18:21:00-07:00</updated><author><name>Donnie Ahmad</name></author><id>tag:None,2018-12-14:/notes-on-a-jmlr-paper.html</id><summary type="html">&lt;h1&gt;Numerical Anaylsis near Singularities in RBF Networks&lt;/h1&gt;
&lt;p&gt;Follow along with the source material &lt;a href="http://www.jmlr.org/papers/volume19/16-210/16-210.pdf"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;1. The parameter space of most learning machines have singular regions where the Fisher information matrices degenerate.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A parameter space is the set of all possible combinations of values for all parameters in a model. &lt;a href="https://en.wikipedia.org/wiki/Parameter_space"&gt;(src …&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1&gt;Numerical Anaylsis near Singularities in RBF Networks&lt;/h1&gt;
&lt;p&gt;Follow along with the source material &lt;a href="http://www.jmlr.org/papers/volume19/16-210/16-210.pdf"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;1. The parameter space of most learning machines have singular regions where the Fisher information matrices degenerate.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A parameter space is the set of all possible combinations of values for all parameters in a model. &lt;a href="https://en.wikipedia.org/wiki/Parameter_space"&gt;(src)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A parameter differs from a variable in a model in that they tend to represent inherent properties of a system. &lt;a href="https://en.wikipedia.org/wiki/Parameter"&gt;(src)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Learning machine examples provided in the paper and brief explanations&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Layered neural networks: used to define a complex, non-linear form of hypotheses using "neurons" that have their input and output mapped through operations called activation functions. &lt;a href="http://deeplearning.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/"&gt;(src)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Normal mixtures: where the joint probability of two random variables is normally distributed. &lt;a href="https://www.jmp.com/support/help/14/normal-mixtures.shtml"&gt;(src)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binomial mixtures&lt;/li&gt;
&lt;li&gt;Bayes networks: represents variables and their "conditional dependencies" in an ordered graphical model. &lt;a href="https://en.wikipedia.org/wiki/Bayesian_network"&gt;(src)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hidden Markov models: a Markov process in an unobserved space. &lt;a href="https://en.wikipedia.org/wiki/Hidden_Markov_Models"&gt;(src)&lt;/a&gt; A Markov chain describes a sequence of events in which the current event relies entirely on the position the previous event created. &lt;a href="https://en.wikipedia.org/wiki/Markov_chain"&gt;(src)&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;Boltzmann machines: a type of stochastic recurrent neural network. &lt;a href="https://en.wikipedia.org/wiki/Boltzmann_machine"&gt;(src)&lt;/a&gt; Stochastic neural networks use random variations to help escape local minima in optimization problems. &lt;a href="https://en.wikipedia.org/wiki/Stochastic_neural_network"&gt;(src)&lt;/a&gt; Recurrent neural networks are where connections between nodes form a directed graph along a sequence. &lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;(src)&lt;/a&gt; A directed graph differs from an undirected graph in that the nodes (or edges) are ordered. &lt;a href="https://en.wikipedia.org/wiki/Directed_graph"&gt;(src)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stochastic context-free grammars: a set of rules that describe all possible strings in a language. &lt;a href="https://en.wikipedia.org/wiki/Context-free_grammar"&gt;(src)&lt;/a&gt; As noted above, stochastic refers to the introduced randomness.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A singular region is where a mathematical object is undefined or where the function "misbehaves." A simple example is that f(x) = 1/x has a singularity at x = 0. [(src)](https://en.wikipedia.org/wiki/Singularity_(mathematics)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fisher information is a measurement of how much information a random variable has about an unknown parameter. &lt;a href="https://en.wikipedia.org/wiki/Fisher_information"&gt;(src)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A matrix "degenerates" in that it becomes a singular matrix. Degenerate and singular matrices are defined as not having an inverse. &lt;a href="https://math.stackexchange.com/questions/792587/what-is-the-difference-between-singular-matrices-and-degenerate-matrices"&gt;(src)&lt;/a&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What this sentence means in its entirety is currently a mystery to me.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Learning dynamics are slowed and trapped in plateaus by singularities in the case of feedforward neural networks.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learning dynamics? Perhaps some numerical measure of how a model acts?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feedforward neural networks? An artificial neural network where connections between nodes do not form a cycle. &lt;a href="https://en.wikipedia.org/wiki/Feedforward_neural_network"&gt;(src)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. The criteria used to select a model and the standard paradigm described by the Cramer-Rao theorem both fail at singularities.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model selection techniques mentioned and brief explanations&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Akaike Information Criterion: estimates the amount of information lost from using a given model to describe some process. &lt;a href="https://en.wikipedia.org/wiki/Akaike_information_criterion"&gt;(src)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bayes Information Criterion: based on the likelihood function but penalizes model complexity. &lt;a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion"&gt;(src)&lt;/a&gt;. Likelihood is a measure of the likelihood of one value given another &lt;a href="https://en.wikipedia.org/wiki/Likelihood_function"&gt;(src)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Minimum Description Length: comparable to Occam's razor. &lt;a href="https://en.wikipedia.org/wiki/Minimum_description_length"&gt;(src)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cramer Rao theorem: describes estimators of a deterministic but unknown parameter. Relates to Fisher information. &lt;a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound"&gt;(src)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. A selection tool that accounts for singularities is the widely applicable Bayesian information criteria (WBIC), which was used to solve the Gaussian process regression case.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;WBIC: A generalized version of the afore-described BIC that can be calculated even without knowing the true distribution of the underlying data. &lt;a href="https://en.wikipedia.org/wiki/Watanabe–Akaike_information_criterion"&gt;(src)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gaussian process regression case: A Gaussian process is a collection of random variables where every combination of the values is normally distributed. This specific tool is used to infer continuous values. It is also known as kriging. &lt;a href="https://en.wikipedia.org/wiki/Gaussian_process"&gt;(src)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;5.&lt;/h3&gt;</content></entry><entry><title>Hi, I'm Daniyal Ahmad, but people call me Donnie</title><link href="/hi-im-daniyal-ahmad-but-people-call-me-donnie.html" rel="alternate"></link><published>2018-12-14T17:58:00-07:00</published><updated>2018-12-14T17:58:00-07:00</updated><author><name>Donnie Ahmad</name></author><id>tag:None,2018-12-14:/hi-im-daniyal-ahmad-but-people-call-me-donnie.html</id><summary type="html">&lt;p&gt;I want this to be a place for me to write "book reports" on textbook chapters and academic papers I read.&lt;/p&gt;
&lt;p&gt;These articles will likely serve as a poor learning resource for others since their primary purpose is for me to learn through summarization.&lt;/p&gt;
&lt;p&gt;Perhaps, these may whet the supposed …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I want this to be a place for me to write "book reports" on textbook chapters and academic papers I read.&lt;/p&gt;
&lt;p&gt;These articles will likely serve as a poor learning resource for others since their primary purpose is for me to learn through summarization.&lt;/p&gt;
&lt;p&gt;Perhaps, these may whet the supposed reader's appetite to read the primary sources.&lt;/p&gt;
&lt;p&gt;Happy holidays!
Donnie&lt;/p&gt;</content></entry></feed>