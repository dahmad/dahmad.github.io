<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Donnie Ahmad</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Donnie Ahmad Atom Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Donnie Ahmad </a></h1>
                <nav><ul>
                    <li><a href="/category/blog.html">Blog</a></li>
                    <li><a href="/category/book-report.html">Book Report</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/notes-on-a-jmlr-paper.html">Notes on a JMLR Paper</a></h1>
<footer class="post-info">
        <abbr class="published" title="2018-12-14T18:21:00-07:00">
                Published: Fri 14 December 2018
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/donnie-ahmad.html">Donnie Ahmad</a>
        </address>
<p>In <a href="/category/book-report.html">Book Report</a>.</p>

</footer><!-- /.post-info --><h1>Numerical Anaylsis near Singularities in RBF Networks</h1>
<p>Follow along with the source material <a href="http://www.jmlr.org/papers/volume19/16-210/16-210.pdf">here</a></p>
<h3>1. The parameter space of most learning machines have singular regions where the Fisher information matrices degenerate.</h3>
<ul>
<li>
<p>A parameter space is the set of all possible combinations of values for all parameters in a model. <a href="https://en.wikipedia.org/wiki/Parameter_space">(src)</a></p>
</li>
<li>
<p>A parameter differs from a variable in a model in that they tend to represent inherent properties of a system. <a href="https://en.wikipedia.org/wiki/Parameter">(src)</a></p>
</li>
<li>
<p>Learning machine examples provided in the paper and brief explanations</p>
<ol>
<li>Layered neural networks: used to define a complex, non-linear form of hypotheses using "neurons" that have their input and output mapped through operations called activation functions. <a href="http://deeplearning.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">(src)</a></li>
<li>Normal mixtures: where the joint probability of two random variables is normally distributed. <a href="https://www.jmp.com/support/help/14/normal-mixtures.shtml">(src)</a></li>
<li>Binomial mixtures</li>
<li>Bayes networks: represents variables and their "conditional dependencies" in an ordered graphical model. <a href="https://en.wikipedia.org/wiki/Bayesian_network">(src)</a></li>
<li>Hidden Markov models: a Markov process in an unobserved space. <a href="https://en.wikipedia.org/wiki/Hidden_Markov_Models">(src)</a> A Markov chain describes a sequence of events in which the current event relies entirely on the position the previous event created. <a href="https://en.wikipedia.org/wiki/Markov_chain">(src)</a> </li>
<li>Boltzmann machines: a type of stochastic recurrent neural network. <a href="https://en.wikipedia.org/wiki/Boltzmann_machine">(src)</a> Stochastic neural networks use random variations to help escape local minima in optimization problems. <a href="https://en.wikipedia.org/wiki/Stochastic_neural_network">(src)</a> Recurrent neural networks are where connections between nodes form a directed graph along a sequence. <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">(src)</a> A directed graph differs from an undirected graph in that the nodes (or edges) are ordered. <a href="https://en.wikipedia.org/wiki/Directed_graph">(src)</a></li>
<li>Stochastic context-free grammars: a set of rules that describe all possible strings in a language. <a href="https://en.wikipedia.org/wiki/Context-free_grammar">(src)</a> As noted above, stochastic refers to the introduced randomness.</li>
</ol>
</li>
<li>
<p>A singular region is where a mathematical object is undefined or where the function "misbehaves." A simple example is that f(x) = 1/x has a singularity at x = 0. [(src)](https://en.wikipedia.org/wiki/Singularity_(mathematics)</p>
</li>
<li>
<p>Fisher information is a measurement of how much information a random variable has about an unknown parameter. <a href="https://en.wikipedia.org/wiki/Fisher_information">(src)</a></p>
</li>
<li>
<p>A matrix "degenerates" in that it becomes a singular matrix. Degenerate and singular matrices are defined as not having an inverse. <a href="https://math.stackexchange.com/questions/792587/what-is-the-difference-between-singular-matrices-and-degenerate-matrices">(src)</a> </p>
</li>
<li>
<p>What this sentence means in its entirety is currently a mystery to me.</p>
</li>
</ul>
<h3>2. Learning dynamics are slowed and trapped in plateaus by singularities in the case of feedforward neural networks.</h3>
<ul>
<li>
<p>Learning dynamics? Perhaps some numerical measure of how a model acts?</p>
</li>
<li>
<p>Feedforward neural networks? An artificial neural network where connections between nodes do not form a cycle. <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">(src)</a></p>
</li>
</ul>
<h3>3. The criteria used to select a model and the standard paradigm described by the Cramer-Rao theorem both fail at singularities.</h3>
<ul>
<li>
<p>Model selection techniques mentioned and brief explanations</p>
<ol>
<li>Akaike Information Criterion: estimates the amount of information lost from using a given model to describe some process. <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">(src)</a></li>
<li>Bayes Information Criterion: based on the likelihood function but penalizes model complexity. <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">(src)</a>. Likelihood is a measure of the likelihood of one value given another <a href="https://en.wikipedia.org/wiki/Likelihood_function">(src)</a></li>
<li>Minimum Description Length: comparable to Occam's razor. <a href="https://en.wikipedia.org/wiki/Minimum_description_length">(src)</a></li>
</ol>
</li>
<li>
<p>Cramer Rao theorem: describes estimators of a deterministic but unknown parameter. Relates to Fisher information. <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">(src)</a></p>
</li>
</ul>
<h3>4. A selection tool that accounts for singularities is the widely applicable Bayesian information criteria (WBIC), which was used to solve the Gaussian process regression case.</h3>
<ul>
<li>
<p>WBIC: A generalized version of the afore-described BIC that can be calculated even without knowing the true distribution of the underlying data. <a href="https://en.wikipedia.org/wiki/Watanabe–Akaike_information_criterion">(src)</a></p>
</li>
<li>
<p>Gaussian process regression case: A Gaussian process is a collection of random variables where every combination of the values is normally distributed. This specific tool is used to infer continuous values. It is also known as kriging. <a href="https://en.wikipedia.org/wiki/Gaussian_process">(src)</a></p>
</li>
</ul>
<h3>5. The error function is a viable alternative to the traditional log-sigmoid function to understand the learning dynamics for multilayer perceptrons (MLPs).</h3>
<h3>6. The averaged learning equations, the learning dynamics near overlap singularities, and mechanism of plateau phenomena near singularities has been studied for radial basis functions, which are a type of feedforward neural networks.</h3>
<h3>7. The <em>natural gradient method</em> is a proposed solution to the problem that standard gradient methods are not <em>Fisher efficient</em> and the gradient descent is not steep enough when there are singularities.</h3>
<h3>8. The difficulty of training deep neural networks using Backpropagation is that it is computationally expensive and presents vanishing gradient problems.</h3>
<ul>
<li>
<p>Backpropagation: </p>
</li>
<li>
<p>Vanishing gradient problem: </p>
</li>
</ul>
<h3>9. One proposed solution is a deep belief network which multilayer Boltzmann machines which are trained layer-by-layer in a greedy fashion.</h3>
<h3>10. "[The] robustness of the training effect [for deep neural networks] cannot be guaranteed, even with a pre-training process."</h3>
<h3>11. It is not clear why large models remain slow despite limited obstacles</h3>
<h3>12. Saddle points may cause pain points.</h3>
<h3>13. Gradient descent can find a band of low critical points, which are local minima of "high quality."</h3>
<h3>14. This paper contends that "points in the elimination singularity are saddles, the points in the overlap singularity are local minima . . . and the generalization error surface near the overlap singularity is very flat."</h3>
<h3>15. The error of deep linear neural networks does not change under a scaling transformation.</h3>
<h3>16. Scaling symmetries are similar to elimination singularities.</h3>
<h3>17. The learning dynamics of deep belief nets, deep convolutional neural networks, and deep multilayer perceptrons may be affected by singularities.</h3>
<h3>18. Singularities are worth studying because of overfitting issues and to improve learning efficiency</h3>
<h3>19. There are two types of singularities in the parameter space of RBF networks: overlap and elimination singularities.</h3>
<h3>20. Dynamics near singularities are studied through simulation, and it is observed that the behavior is similar between RBF networks, multilayer perceptrons and Gaussian mixtures (and possibly otherfeedforward neural networks)</h3>
<h3>21. An RBF Network with k hidden units is the function of x and theta where x is a point in a real coordinate space that denotes the input vector and theta represents all of the parameters of the model.</h3>
<h3>22. The function equals the sum from 1 to k of the weight of the neuron times the Guassian function of x and J (the center vector for the neuron which is also a point in a real coordinate space).</h3>
<h3>23. If two hidden units overlap, then the first weight times the Gaussian function of x and the first parameter plus the second weight times the Gaussian function of x and the second parameter equals the sum of the weights times the Gaussian function of x and the first parameter regardless of what the weights actually are.</h3>
<h3>24. Therefore, w equals the sum of the two parameters even if you don't know the individual weights.</h3>
<h3>25. An overlap singularity is the set of all parameters where the two center vectors are equal.</h3>
<h3>26. An elimination singularity is the set of all parameters where the first weight equals zero.</h3>
<h3>27. This paper explicitly describes the Fisher information matrix for the RBF network, uses average learning equations to examine RBF learning dynamics, and runs numerical simulations to see what happens near singularities.</h3>
<h3>28. The Fisher information matrix is an indicator of a singularity.</h3>
<h3>29. For regressions, an unknown teacher function generates y from x plus some additive noise.</h3>
<h3>30. The training input has a Gaussian distribution and a covariance matrix.</h3>
<h3>31. The covariance matrix does not change the analytical results so it can be set to the identity matrix.</h3>
<h3>32. For RBFs, the Fisher information matrix is inner product of is the delta (partial derivative?) of the function of x and theta divided by the delta of theta for two points.</h3>
<h3>33. This can be further expanded into a much larger matrix where the set of complex numbers Ji and Jj equal some multiplication involving a lower case sigma that I cannot parse.</h3>
<ul>
<li>A lot going on here that I don't understand. . . .</li>
</ul>
<h3>34. Near singularities, the condition value of the matrix is large and its inverse is near zero.</h3>
<h3>35. So the inverse of the condition value can measure nearness to a singularity.</h3>
<h3>36. Using the inverse of the Fisher information matrix as a weight overcomes the influence of singularities in the natural gradient descent method.</h3>
<h3>37. Obtaining the analytical form of the Fisher information matrix improves the natural gradient descent algorithms moving forward in addition to being fundamental to the following numerical analysis.</h3>
<h3>38. Artificial experiments were analyzed for low and medium dimensioned examples given a known input distribution.</h3>
<h3>39. Batch made learning dynamics are similar to those using the averaged learning equations.</h3>
<h3>40. ALEs don't depend on specific data.</h3>
<h3>41. ALEs are ordinary differential equations and be solved to obtain student parameters.</h3>
<h3>42. The function-naught of x equals the function of x theta-naught plus some Gaussian additive noise that is uncorrelated with training input x.</h3>
<h3>43. Furthermore, this equals the sum of v times the Gaussian function of x and t plus that noise.</h3>
<h3>44. The analytical form of ALEs is an extremely long equation that I am not prepared to understand. . .</h3>
<h3>45. There is a generalized error equation defined here as well.</h3>
<h3>46. Investigating a model with two hidden units is enough to capture thes essence.</h3>
<h3>47. This can be compared to general cases of RBF networks; all of which is "obtained by solving ALEs for the given teacher parameters and initial student parameters."</h3>
<h3>48. Analyzing a case where the teacher and student models have two hidden units.</h3>
<h3>49. Lower-case sigma refers to the spread constant.</h3>
<h3>50. Focus on an input x with dimension 1 because the global minimum is the point where generalization error is 0.</h3>
<h3>51. Part of the student parameters are kept invariable in order to create 3D visuals.</h3>
<h3>52. Two teacher model parameters are set to another initial parameter and only J1 and J2 are modified for overlap singularities.</h3>
<h3>53. A different set of weights are fixed in other cases.</h3>
<h3>54. In general, these are only two variable parameters.</h3>
<h3>55. Plotting learning trajectories is accomplished through the generalization error surface in a 3D figure.</h3>
<h3>56. Choosing only one teacher function is done to illustrate that RBF networks can be affected by the singularities under different initial states.</h3>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/hi-im-daniyal-ahmad-but-people-call-me-donnie.html" rel="bookmark"
                           title="Permalink to Hi, I'm Daniyal Ahmad, but people call me Donnie">Hi, I'm Daniyal Ahmad, but people call me Donnie</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2018-12-14T17:58:00-07:00">
                Published: Fri 14 December 2018
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/donnie-ahmad.html">Donnie Ahmad</a>
        </address>
<p>In <a href="/category/blog.html">Blog</a>.</p>

</footer><!-- /.post-info -->                <p>I want this to be a place for me to write "book reports" on textbook chapters and academic papers I read.</p>
<p>These articles will likely serve as a poor learning resource for others since their primary purpose is for me to learn through summarization.</p>
<p>Perhaps, these may whet the supposed …</p>
                <a class="readmore" href="/hi-im-daniyal-ahmad-but-people-call-me-donnie.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>